{
 "cells": [
  {
   "cell_type": "code",
   "id": "b6574066-cb38-4300-bcdb-bdfce9bbf63b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T13:13:51.455323Z",
     "start_time": "2025-12-19T13:13:51.452937Z"
    }
   },
   "source": [
    "#https://www.geeksforgeeks.org/python/multiprocessing-python-set-1/\n",
    "#https://docs.python.org/3/library/multiprocessing.html\n",
    "#https://docs.python.org/3/library/os.html\n",
    "#https://stackoverflow.com/questions/18300785/python-top-n-word-count-why-multiprocess-slower-then-single-process\n",
    "#https://docs.python.org/3/library/concurrent.futures.html"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "487a283a-38ef-4618-a809-c3be1bb898f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T13:13:51.477589Z",
     "start_time": "2025-12-19T13:13:51.474029Z"
    }
   },
   "source": [
    "import time  #Used to measure execution time (latency) for performance comparison.\n",
    "import multiprocessing  #Required to detect the number of available CPU cores.\n",
    "import os  #Used for file system operations, such as checking if the text file exists.\n",
    "from collections import Counter  #specialized dictionary for efficient counting and merging of word frequencies.\n",
    "from concurrent.futures import ProcessPoolExecutor  #Manages a pool of processes for parallel execution.\n",
    "# Importing relevant libraries for distributed processing and file handling.\n",
    "\n",
    "try:\n",
    "    # Importing the worker function from an external file to avoid 'BrokenProcessPool' errors in Windows/Jupyter.\n",
    "    from my_worker import clean_and_count\n",
    "    print(\"Success: Imported 'clean_and_count' from external file.\")\n",
    "except ImportError:\n",
    "    print(\"Error: Could not find 'my_worker.py'. Make sure the file is in the same directory.\")\n",
    "\n",
    "# Define the filename \n",
    "FILENAME = 'pg2701.txt'\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Imported 'clean_and_count' from external file.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "raw",
   "id": "263526e3-2d93-4c1f-906c-c5df4b8e61d2",
   "metadata": {},
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Worker function to be executed by parallel processes\n",
    "def clean_and_count(text_chunk):\n",
    "    # Create a translation table to remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Clean data: Remove punctuation and convert to lowercase\n",
    "    clean_text = text_chunk.translate(translator).lower()\n",
    "    \n",
    "    # Split the text into individual words\n",
    "    words = clean_text.split()\n",
    "    \n",
    "    # Return a Counter object (dictionary of word frequencies)\n",
    "    return Counter(words)\n",
    "This is the code for cleaning the data and is importet from a sperate file to minimazie errors when multithreading"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd337aab-f88a-4343-8cba-697504af8878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T13:13:51.548558Z",
     "start_time": "2025-12-19T13:13:51.502506Z"
    }
   },
   "source": [
    "full_text = \"\"\n",
    "# Check if the file exists at the specified path\n",
    "if os.path.exists(FILENAME):\n",
    "    try:\n",
    "        # Open the file using 'utf-8-sig' to handle Byte Order Mark (BOM) correctly\n",
    "        with open(FILENAME, 'r', encoding='utf-8-sig') as f:\n",
    "            original_text = f.read()\n",
    "            \n",
    "            # Multiply the text by 100 to simulate a larger dataset.\n",
    "            # This increases the CPU load to better demonstrate the performance\n",
    "            # benefits of multiprocessing vs single-threading.\n",
    "            full_text = original_text * 100\n",
    "            \n",
    "        print(f\"File loaded successfully. Total characters: {len(full_text)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error: Could not read the file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File '{FILENAME}' not found. Please check the filename and directory.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully. Total characters: 123822500\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "a8473ac2-99f0-4f3a-bfec-14c95d6f6264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T13:13:51.627924Z",
     "start_time": "2025-12-19T13:13:51.563313Z"
    }
   },
   "source": [
    "if full_text:\n",
    "    # Determine the number of available CPU cores (logical processors)\n",
    "    # This defines how many parallel processes we can efficiently run.\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Calculate the size of each chunk to ensure an even workload distribution.\n",
    "    # We use max() to ensure chunk_size is at least 1 to avoid division errors.\n",
    "    chunk_size = max(len(full_text) // num_workers, 1)\n",
    "    \n",
    "    # Create the list of chunks (Data Partitioning)\n",
    "    # The large text string is sliced into smaller segments based on the calculated size.\n",
    "    chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "    \n",
    "    print(f\"Number of CPU cores available: {num_workers}\")\n",
    "    print(f\"Data partitioning complete: Text split into {len(chunks)} chunks ready for processing.\")\n",
    "\n",
    "else:\n",
    "    print(\"No text loaded.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores available: 12\n",
      "Data partitioning complete: Text split into 13 chunks ready for processing.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "7dd67db0-ae22-4669-b077-2299dd816319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T13:14:11.253178Z",
     "start_time": "2025-12-19T13:13:51.650361Z"
    }
   },
   "source": [
    "start_seq = time.time()\n",
    "# Initialize an empty Counter to aggregate word frequencies\n",
    "seq_counts = Counter()\n",
    "\n",
    "# Loop through each data chunk sequentially\n",
    "# In a single-threaded approach, the CPU processes one chunk at a time.\n",
    "# The code blocks (waits) until the current chunk is finished before moving to the next.\n",
    "for chunk in chunks:\n",
    "    # Clean the data and count words for the current chunk\n",
    "    seq_counts.update(clean_and_count(chunk))\n",
    "\n",
    "end_seq = time.time()\n",
    "time_seq = end_seq - start_seq\n",
    "\n",
    "print(f\"Single-thread duration: {time_seq:.4f} seconds\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-thread duration: 19.5984 seconds\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "326902e6-86f3-4e01-beae-8b9e8f093478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T13:14:14.635091Z",
     "start_time": "2025-12-19T13:14:11.332811Z"
    }
   },
   "source": [
    "start_multi = time.time()\n",
    "# Initialize a counter to aggregate results from all processes\n",
    "multi_counts = Counter()\n",
    "# The 'if __name__' block is mandatory on Windows to prevent recursive process spawning.\n",
    "if __name__ == '__main__':\n",
    "    # ProcessPoolExecutor manages a pool of independent worker processes.\n",
    "    # Unlike threading, this bypasses Python's Global Interpreter Lock (GIL),\n",
    "    # allowing true parallel execution on multiple CPU cores.\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # The Map Step:\n",
    "        # Distribute the 'clean_and_count' function across the list of data chunks.\n",
    "        # The executor automatically assigns chunks to available idle workers.\n",
    "        results = executor.map(clean_and_count, chunks)\n",
    "        # The Reduce/Aggregation Step:\n",
    "        # Collect results from workers as they finish and merge them into the main counter.\n",
    "        for res in results:\n",
    "            multi_counts.update(res)\n",
    "\n",
    "end_multi = time.time()\n",
    "time_multi = end_multi - start_multi\n",
    "\n",
    "print(f\"Multiprocessing duration: {time_multi:.4f} seconds\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiprocessing duration: 3.2966 seconds\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "raw",
   "id": "ce2fdf06-4a6d-45a2-a752-560e6470426e",
   "metadata": {},
   "source": [
    "As we can see from the results, the multiprocessing approach gave us a speedup of 7 times. This confirms that for CPU-intensive tasks with large datasets, distributing the work across multiple cores is more efficient. It bypasses the Global Interpreter Lock and allows true parallel execution. However, we must always consider the overhead cost of spawning processes, which is why single-threading can sometimes be faster for smaller tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
